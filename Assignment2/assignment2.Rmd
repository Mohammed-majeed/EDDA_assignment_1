---
title: "Assignment 2"
author: "Group 61, Ikrame Zirar, Mohammed Majeed, Sergio Alejandro Gutierrez Maury"
date: "`r Sys.Date()`"
output: pdf_document
highlight: tango
editor_options: 
  markdown: 
    wrap: sentence
---

```{r echo=FALSE}
options(digits=3)
```

## Excersice 1

**A)** The dataset "treeVolume" contains a response variable, namely "Volume", and several explanatory variables, including "type", "height", and "diameter".
To investigate the impact of tree type on volume, we conducted ANOVA using "Volume" as the response variable and "type" as the sole explanatory variable.
The p-value from the ANOVA table indicates that there is no significant effect of tree type on tree volume.

```{r}
# Load the dataset
tree_data <- read.csv("treeVolume.txt", header = TRUE, sep = "")
# Perform t-test
model_aov <- aov(volume ~ type, data = tree_data)
summary(model_aov)
```

We conducted a t-test to compare the means of these two sample groups.the p-value of the t-test indicates that the type of tree does not have a significant impact on its volume.

```{r}
# Perform t-test
t_test <- t.test(volume ~ type, data = tree_data)
t_test
```

The output of aggregate gives us the estimated volumes for the two tree types

```{r}
# Estimate the volumes for the two tree types
aggregate(tree_data$volume, by = list(tree_data$type), mean)
```

**b)** To include diameter and height as explanatory variables into the analysis and investigate whether the influence of diameter and height on volume is similar for both tree types.

```{r}
# Fit a linear model with diameter and height as explanatory variables
model_lm <- lm(volume ~ diameter + height + type, data = tree_data)
summary(model_lm)
```

The below two ANOVA tables to investigate whether the influence of diameter and height on volume is similar for both tree types.
In both cases, the interaction term is not significant, indicating that the influence of diameter and height on volume is similar for both tree types.

```{r}
# Perform ANOVA to investigate the influence of diameter on volume for both tree types
model_aov_diameter <- aov(volume ~ diameter * type, data = tree_data)
summary(model_aov_diameter)
```

```{r}
# Perform ANOVA to investigate the influence of height on volume for both tree types
model_aov_height <- aov(volume ~ height * type, data = tree_data)
summary(model_aov_height)
```

**c)**

The correlation coefficient is 0.519, indicating a moderate positive linear relationship between diameter and height of beech trees.
The p-value is less than 0.05, indicating that there is strong evidence of a significant correlation between diameter and height of beech trees.

Whereas the correlation coefficient of oak trees is -0.116, indicating a weak negative linear relationship between diameter and height of oak trees.
The p-value is greater than 0.05, indicating that there is not enough evidence to reject the null hypothesis of no correlation between diameter and height of oak trees.

```{r}
par(mfrow=c(1, 2))
plot(tree_data[tree_data$type == "beech", "diameter"],
     tree_data[tree_data$type == "beech", "height"], 
     xlab =  "beech * diameter", ylab = "beech * height")
plot(tree_data[tree_data$type == "oak", "diameter"],
     tree_data[tree_data$type == "oak", "height"], 
     xlab =  "oak * diameter", ylab = "oak * height")

cor.test(tree_data[tree_data$type == "beech", "diameter"],
         tree_data[tree_data$type == "beech", "height"])
cor.test(tree_data[tree_data$type == "oak", "diameter"], 
         tree_data[tree_data$type == "oak", "height"])
```

Using the results from b), we can investigate how diameter, height, and type influence volume.
To predict the volume for a tree with the overall average diameter and height.

```{r}
# Calculate the overall average diameter and height
avg_diameter <- mean(tree_data$diameter)
avg_height <- mean(tree_data$height)

# Predict the volume for a tree with the overall average diameter and height
predict(model_lm, newdata = data.frame(diameter = avg_diameter, height = avg_height, type = "beech"), interval = "confidence")
```

**d)** It seems like there may be a natural relationship between the volume of a tree and its height and diameter.
One possible transformation to consider is taking the logarithm of both height and diameter to create new variables, which may better capture the relationship with volume.

Both models have high R-squared values, indicating that they explain a large proportion of the variation in the response variable.
However, the first model has a slightly higher R-squared value of 0.977 compared to the second model's (with no transformation) R-squared value of 0.951.
This suggests that the first model may be a slightly better fit for the data.

```{r}

# fit a linear model with the transformed variables
transformed_model <- lm(log(volume) ~ log(tree_data$height) + log(tree_data$diameter) + type, data=tree_data)
# print the summary of the model to check the results
summary(transformed_model);

# fit a linear model with the transformed variables
model <- lm(volume ~ tree_data$height + tree_data$diameter + type, data=tree_data)
# print the summary of the model to check the results
summary(model);

```

## Excersice 2

**A)**

It can be seen from the box plots, that there are some outliers that might be
influential, and from the leverage vs residuals plots we can see that, this is
indeed the case.


```{r}
data <- read.csv("expensescrime.txt", header = TRUE, sep = " ")
# scatterplots
par(mfrow=c(3,2), mar=c(0.1,0.1,2,2))

boxplot(data$bad, data = data, notch = TRUE, varwidth = TRUE, main = "Bad")
boxplot(data$crime, data = data, notch = TRUE, varwidth = TRUE, main = "Crime")
boxplot(data$lawyers, data = data, notch = TRUE, varwidth = TRUE, main = "Lawyers")
boxplot(data$employ, data = data, notch = TRUE, varwidth = TRUE, main = "Employ")
boxplot(data$pop, data = data, notch = TRUE, varwidth = TRUE, main = "pop")
boxplot(data$expend, data = data, notch = TRUE, varwidth = TRUE, main = "Expend")
```

```{r}
par(mfrow=c(2,2), mar=c(2,1,2,2))
library(MASS)
rlm_fit1 <- lm(data$expend~data$bad, data=data)
rlm_fit2 <- lm(data$expend~data$crime, data=data)
rlm_fit3 <- lm(data$expend~data$lawyers, data=data)
rlm_fit4 <- lm(data$expend~data$employ, data=data)
rlm_fit5 <- lm(data$expend~data$pop, data=data)
plot(rlm_fit1)
plot(rlm_fit2)
plot(rlm_fit3)
plot(rlm_fit4)
plot(rlm_fit5)

```

By looking at the correlation matrix, it can be seen that there are some multicollinearity
problems, since the variable "bad" is highly correlated with other independent variables.

```{r}
library("reshape2")
library('ggplot2')
# calculate correlation matrix
cor_matrix <- cor(data[, c("bad", "crime", "lawyers", "employ", "pop")])

# plot correlation matrix
ggplot(data = reshape2::melt(cor_matrix)) +
  geom_tile(aes(x = Var1, y = Var2, fill = value)) +
  scale_fill_gradient2(low = "blue", high = "red", midpoint = 0,
                       name = "Correlation") +
  theme_minimal() +
  labs(title = "Correlation Matrix of Explanatory Variables")

round(cor_matrix,2)
pairs(cor_matrix)

```

**B)**


The setp-up method selects as best model: $ \hat{expend} = \beta_{0} + \beta_{1} \cdot bad + \beta_{2} \cdot  lawyers + \beta_{3} \cdot  employ + \beta_{4} \cdot  pop$ 

where all coefficients are significant with at least $5\%$ level.

```{r}
library(MASS)

# fit full model
full_model <- lm(expend ~ bad + crime + lawyers + employ + pop, data=data)

# step-up method to find best model
full_model.step <- stepAIC(full_model, direction="both")

summary(full_model.step)
```

**C)** pending question: can you improve the interval?

The interval is: $ (-192.8264, 805.6644)$

```{r}
# create new data frame with hypothetical values
new_data <- data.frame(bad=50, crime=5000, lawyers=5000, employ=5000, pop=5000)

# predict expend using selected model
pred <- predict(full_model.step, newdata=new_data, interval="prediction", level=0.95)


pred
```

**D)**

Comparing the lasso model with the step-up model, the lasso model set the variables
"bad" and "crime" to zero, which means that those variables are not important.
As a result, we end up with a much simpler model.
```{r}
set.seed(73) #sheldon prime !
library(glmnet)
x <- as.matrix(data[, c("bad", "crime", "lawyers", "employ", "pop")])
y <- data$expend
train=sample(1:nrow(x),0.67*nrow(x)) # train by using 2/3 of the data
x.train=x[train,]; y.train=y[train] # data to train
x.test=x[-train,]; y.test=y[-train] # data to test the prediction quality


lasso.mod=glmnet(x.train,y.train,alpha=1)
cv.lasso=cv.glmnet(x.train,y.train,alpha=1,type.measure='mse')
plot(lasso.mod,label=T,xvar="lambda") #have a look at the lasso path
plot(cv.lasso) # the best lambda by cross-validation
plot(cv.lasso$glmnet.fit,xvar="lambda",label=T)
lambda.min=cv.lasso$lambda.min; lambda.1se=cv.lasso$lambda.1se
coef(lasso.mod,s=cv.lasso$lambda.min) #betaâ€™s for the best lambda
y.pred=predict(lasso.mod,s=lambda.min,newx=x.test) #predict for test
mse.lasso=mean((y.test-y.pred)^2) #mse for the predicted test rows

```

## Excersice 3

**A)**

```{r}
# install.packages("rms",dependencies = TRUE)
#install.packages("Hmisc")
# 
library(ggplot2);
library(Hmisc);
library(rms);
library(rmsb);

```

```{r}
data_titanic <- read.table("titanic.txt", header=TRUE)

plot(xtabs(~Survived + PClass + Sex, data_titanic))

```

```{r}
options(prType='html')
v <- c('PClass','Survived','Age','Sex')
titanic <- data_titanic[, v]
describe(titanic)
```

```{r}
# spar(ps=4,rt=3)spar
dd <- datadist(titanic)
# describe distributions of variables to rms
options(datadist='dd')
s <- summary(Survived ~ Age + Sex + PClass , data=titanic)
plot(s, main='', subtitles=FALSE)

```

```{r}
model  <- glm(Survived ~ PClass + Age + Sex, data = data_titanic, family = binomial())
summary(model)
```

we can exponentiate their coefficients to get the odds ratios for survival.
For example, the odds ratio for PClass2nd is exp(-1.29196) = 0.274, which suggests that passengers in second-class were 0.274 times as likely to survive as passengers in first-class.
Similarly, the odds ratio for Age is exp(-0.03918) = 0.962, which means that for each one-unit increase in age, the odds of survival decrease by a factor of 0.962.
The odds ratio for Sexmale is exp(-2.63136) = 0.072, which suggests that males were 0.072 times as likely to survive as females.

**3C)**

We could use Logistic Regression to model the probability of a certain passenger surviving or not. To evaluate the model, we could use $R^2$ or Accuracy. To implement the model, we would need to clean the dataset, handling missing values, encoding the categorical variables, and normalizing the numerical variables.

## Excersice 4

**A)**

We check for correlation between all pairs of variables.The plot shows that there is no correlation.

We perform Poisson regression and find that \texttt{oligarchy}, \texttt{pollib} and \texttt{parties} have a significant effect on\texttt{miltcoup}, because their p-values are \<0.05.

```{r}
data = read.table(file = "coups.txt", header = TRUE)
pairs(data[,-1])
```

```{r}
glmcoups = glm(miltcoup ~ oligarchy + pollib + parties + pctvote + popn + size + numelec + numregim, family = poisson, data = data)
summary(glmcoups)
```

**B)**

We will use the step-down approach to reduce the number of explanatory variables.
This means we keep the variables that have the most significant effect.
Analyzing the \texttt{summary} in a), we iterate through and remove the variables with the highest p-values.
We end up with \texttt{oligarchy}, \texttt{pollib} and \texttt{parties}.
Comparing the results to a), the step down approach model is ......

```{r}
glmcoups2 = glm(miltcoup~oligarchy+pollib+parties, family = poisson, data = data)
summary(glmcoups2)
```

**C)** #Using the model from b), predict the number of coups for a hypothetical country for all the three levels of political liberalization and the (overall) averages of all the other (numerical) characteristics.
Comment on your findings.

The predicted average of coups per country increases as the policitical liberalization decreases.???

```{r}
avg1 =0.25138+0.09262*mean(data$oligarchy)-0.57410*0+0.02206*mean(data$parties)
avg2 =0.25138+0.09262*mean(data$oligarchy)-0.57410*1+0.02206*mean(data$parties)
avg3 =0.25138+0.09262*mean(data$oligarchy)-0.57410*2+0.02206*mean(data$parties)
avg =c(exp(avg1), exp(avg2), exp(avg3))
avg1; avg2; avg3; avg
```
